# æ–°æ¨¡å‹æ·»åŠ æ‰‹å†Œ

<aside>
ğŸ’¡ ä»¥pixart-alphaä¸ºä¾‹ï¼Œå¹¶è¡Œç‰ˆæœ¬æ¨¡å‹æ”¯æŒçš„å…¨è¿‡ç¨‹å¦‚ä¸‹
</aside>

<aside>
âš ï¸ æ³¨æ„ï¼Œåœ¨`__call__`ä»¥åŠ`forward`ä¸­ç›´æ¥ä½¿ç”¨çˆ¶ç±»æä¾›çš„å‡½æ•°è¿›è¡Œä¿®æ”¹æ—¶è¯·å°å¿ƒï¼Œè‹¥çˆ¶ç±»æä¾›çš„å‡½æ•°æ— æ³•æ»¡è¶³éœ€æ±‚ï¼Œåˆ™éœ€è¦è‡ªè¡Œé‡è½½è¯¥å‡½æ•°ä»è€Œä½¿ä¹‹ä¸æ¨¡å‹åŒ¹é…
</aside>

# 0.å‘½åè§„èŒƒ

- xDiTçš„æ‰€æœ‰modelï¼Œschedulerï¼Œlayerçš„**æ–‡ä»¶å**åº”ä¸æ¥æºåº“diffuserä¸­çš„åç§°ä¿æŒä¸€è‡´
- xDiTçš„Pipelineç±»wrapperï¼Œä¸ºäº†å’Œdiffusers pipelineå®Œå…¨é€‚é…ï¼Œå…¶åç§°åº”ä¸º`xFuser+åŸå` ï¼Œè€Œä¸å¸¦æœ‰`Wrapper`åç¼€ï¼Œå¦‚pixart-alphaåœ¨diffusersä¸­çš„pipelineåä¸º`PixArtAlphaPipeline`ï¼Œåˆ™åœ¨xDiTä¸­åç§°åº”ä¸º`xFuserPixArtAlphaPipeline`
- é™¤Pipelineç±»ä»¥å¤–çš„å…¶ä»–ç±»çš„wrapperåç§°åº”ä¸º`xFuser+åŸå+Wrapper` ï¼Œå¦‚pixart-alphaçš„backboneä¸º`PixArtTransformer2D`ï¼Œæ­¤backboneåœ¨xDiTä¸­çš„wrapperåç§°åº”ä¸º`xFuserPixArtTransformer2DWrapper`

ä¸‹å›¾å±•ç¤ºäº†xDiTé¡¹ç›®ä¸­ä¸åŒClassçš„è°ƒç”¨å…³ç³»ã€‚å¦‚æœæ–°åŠ ä¸€ä¸ªPixArtæ¨¡å‹ï¼Œå¢åŠ çš„ç±»ç”¨çº¢æ¡†åœˆå‡ºã€‚

![class_structure.png](../../assets/developer/class_structure.png)

# 1.pipeline class

pipelinesæ–‡ä»¶ç›®å½•ä½äº`xfuser/model_executor/pipelines`ï¼Œåœ¨è¯¥ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªä¸diffusersåº“ä¸­å¯¹åº”pipelineåŒåçš„æ–‡ä»¶(æ­¤å¤„ä¸ºpipeline_pixart_alpha.py)ï¼Œåœ¨é‡Œé¢ç¼–å†™ä¸€ä¸ªpipeline wrapperç±»`xFuserPixArtAlphaPipeline`ã€‚è¿™ä¸ªç±»éœ€è¦ä½¿ç”¨è£…é¥°å™¨`xFuserPipelineWrapperRegister.register`ï¼Œè¯¥è£…é¥°å™¨ä¼šå°†æ­¤pipeline wrapperä¸åŸå§‹pipelineçš„å¯¹åº”å…³ç³»è¿›è¡Œæ³¨å†Œï¼Œæ–¹ä¾¿ä¹‹åçš„è‡ªåŠ¨å¹¶è¡ŒåŒ–ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
@xFuserPipelineWrapperRegister.register(PixArtAlphaPipeline)
class xFuserPixArtAlphaPipeline(xFuserPipelineBaseWrapper):

   @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        engine_config: EngineConfig,
        **kwargs,
    ):
        pipeline = PixArtAlphaPipeline.from_pretrained(
            pretrained_model_name_or_path, **kwargs
        )
        return cls(pipeline, engine_config)

    @torch.no_grad()
    @xFuserPipelineBaseWrapper.enable_data_parallel
    @xFuserPipelineBaseWrapper.check_to_use_naive_forward
    def __call__(...
```

pipeline wrapperä¸­ï¼Œä»…éœ€è¦å®ç°ä¸¤ä¸ªå‡½æ•°ï¼Œ`from_pretrained`ç”¨ä»¥å°†å‚æ•°è½¬å‘åˆ°åŸpipelineç±»(æ­¤å¤„ä¸ºdiffuserä¸­çš„`PixArtAlphaPipeline`)çš„`from_pretrained`ï¼Œè·å–åˆ°ä¸€ä¸ªåŸpipelineå¯¹è±¡ï¼Œå¹¶å°†å…¶ä¼ è¿›`cls.__init__`ã€‚éšåé€å±‚initçš„è¿‡ç¨‹ä¸­ä¼šé€æ¸å°†å…¶ç»„åˆ†è¿›è¡Œå¹¶è¡ŒåŒ–ï¼Œæ­¤ç§æ–¹å¼å®Œå…¨å…¼å®¹diffusersæ¥å£ã€‚

å°†åŸå§‹xFuserPixArtAlphaPipelineä¸­çš„æ‰€æœ‰`@property`å¤åˆ¶ç²˜è´´åˆ°`xFuserPixArtAlphaPipeline`ä¸­ã€‚

æœ€å¤§çš„æ”¹åŠ¨æ˜¯`__call__` æ–¹æ³•ï¼Œé¦–å…ˆä½¿ç”¨`xFuserPipelineBaseWrapper`ä¸­çš„ä¸¤ä¸ªè£…é¥°å™¨åŒ…è£…å®ƒï¼Œè¿™æ˜¯æ˜¯å¿…é¡»çš„ä¸”é¡ºåºä¸å¯å˜ï¼Œå…¶ä½œç”¨å¦‚ä¸‹ï¼š

- `enable_data_parallel`ï¼šå¯ç”¨æ•°æ®å¹¶è¡Œ(dp)ï¼Œä¼šåœ¨__call__ä¹‹å‰è‡ªåŠ¨è¯»å–dpç›¸å…³é…ç½®ä¸è¾“å…¥promptsï¼Œå½“prompå¤šä¸ªæ—¶å€™ï¼Œä¼šåˆ†é…åˆ°ä¸åŒçš„dp rankä¸Šæ‰§è¡Œã€‚å¦‚æœè¾“å…¥promptåªæœ‰ä¸€ä¸ªï¼Œåˆ™ä¸å‘æŒ¥ä½œç”¨ã€‚
- `check_to_use_naive_forward`ï¼šè¿›è¡Œå¹¶è¡Œæ¡ä»¶æ£€æµ‹ã€‚è‹¥ä»…enableäº†data parallelï¼Œåˆ™ç›´æ¥ä½¿ç”¨è¯¥è£…é¥°å™¨å¯¹è¾“å…¥promptsè¿›è¡Œæœ´ç´ forwardæ¨ç†

<aside>
ğŸ’¡ è£…é¥°å™¨çš„é¡ºåºä¸å¯äº¤æ¢ï¼Œå¦åˆ™åœ¨ä½¿ç”¨æœ´ç´ forwardæ—¶data parallelå°†æ— æ³•ä½¿ç”¨ã€‚

</aside>

ç„¶åå¯¹`__call__`ä¸­ä»£ç åšä¿®æ”¹ã€‚

## 1.1.__call__æ”¹åŠ¨

`__call__`ä¸­ä»£ç é€»è¾‘æ˜¯åœ¨diffusersåº“å¯¹åº”pipelineçš„`__call__`å‡½æ•°ä¸­æ²¿è¢­è€Œæ¥çš„ï¼Œéœ€è¦ç°å°†pipelineçš„`__call__`å‡½æ•°å¤åˆ¶åˆ°å¯¹åº”wrapperçš„`__call__`ä¸­ï¼Œå†è¿›è¡Œè¿›ä¸€æ­¥ä¿®æ”¹

<aside>
ğŸ“ ä¾‹å¦‚å¯¹äºpixart-alphaæ¥è¯´ï¼Œéœ€è¦å…ˆå°†`diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py`ä¸­`PixArtAlphaPipeline`ç±»çš„`__call__`å¤åˆ¶ä¸º`xfuser/model_executor/pipelines/pipeline_pixart_alpha.py`ä¸­`xFuserPixArtAlphaPipeline`ç±»çš„`__call__`

</aside>

1. encode input promptä¹‹å‰ï¼Œè®¡ç®—å‡ºbatch sizeä¹‹åã€‚ä½¿ç”¨æœ¬æ¬¡æ¨ç†çš„é•¿å®½å’Œbatch sizeè°ƒç”¨`set_input_parameters`æ¥å¯¹æœ¬æ¬¡forwardçš„è¾“å…¥ä¿¡æ¯è¿›è¡Œè®¾ç½®ï¼Œä»è€Œè®¡ç®—å‡ºå„ç§è¿è¡Œæ—¶åŸæ•°æ®ï¼Œä¸ºæ­£å¼forwardåšå‡†å¤‡

    ```python
            ...
            # 2. Default height and width to transformer
            if prompt is not None and isinstance(prompt, str):
                batch_size = 1
            elif prompt is not None and isinstance(prompt, list):
                batch_size = len(prompt)
            else:
                batch_size = prompt_embeds.shape[0]
            device = self._execution_device

            # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
            # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
            # corresponds to doing no classifier free guidance.
            do_classifier_free_guidance = guidance_scale > 1.0
    **# -------------------------- ADDED BELOW ------------------------------**
            #* set runtime state input parameters
            get_runtime_state().set_input_parameters(
                height=height,
                width=width,
                batch_size=batch_size,
                num_inference_steps=num_inference_steps,
            )
    **# -------------------------- ADDED ABOVE ------------------------------**
            # 3. Encode input prompt
            (
                prompt_embeds,
                prompt_attention_mask,
                negative_prompt_embeds,
                negative_prompt_attention_mask,
            ) = self.encode_prompt(
                prompt,
                do_classifier_free_guidance,
                negative_prompt=negative_prompt,
                num_images_per_prompt=num_images_per_prompt,
                device=device,
                prompt_embeds=prompt_embeds,
                negative_prompt_embeds=negative_prompt_embeds,
                prompt_attention_mask=prompt_attention_mask,
                negative_prompt_attention_mask=negative_prompt_attention_mask,
                clean_caption=clean_caption,
                max_sequence_length=max_sequence_length,
            )
            ...
    ```

2. ä¿®æ”¹`do_classifier_free_guidance`çš„æƒ…å†µä¸‹çš„`prompt_embeds`&`prompt_attention_mask`åˆ’åˆ†ï¼Œåˆ¤å®šsplit batchçš„æƒ…å†µ

    ```python
            ...
            # 3. Encode input prompt
            (
                prompt_embeds,
                prompt_attention_mask,
                negative_prompt_embeds,
                negative_prompt_attention_mask,
            ) = self.encode_prompt(
                prompt,
                do_classifier_free_guidance,
                negative_prompt=negative_prompt,
                num_images_per_prompt=num_images_per_prompt,
                device=device,
                prompt_embeds=prompt_embeds,
                negative_prompt_embeds=negative_prompt_embeds,
                prompt_attention_mask=prompt_attention_mask,
                negative_prompt_attention_mask=negative_prompt_attention_mask,
                clean_caption=clean_caption,
                max_sequence_length=max_sequence_length,
            )

    **#! ---------------------------------------- MODIFIED BELOW ----------------------------------------**
            # * dealing with cfg degree
            if do_classifier_free_guidance:
                (
                    prompt_embeds,
                    prompt_attention_mask,
                ) = self._process_cfg_split_batch(
                    prompt_embeds,
                    prompt_attention_mask,
                    negative_prompt_embeds,
                    negative_prompt_attention_mask
                )

            #! ORIGIN
            # if do_classifier_free_guidance:
            #     prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            #     prompt_attention_mask = torch.cat([negative_prompt_attention_mask, prompt_attention_mask], dim=0)
    **#! ---------------------------------------- MODIFIED ABOVE ----------------------------------------**
    				...
    ```

3. ä»ç„¶æ˜¯å¯¹classifier_free_guidanceå’Œsplit batchçš„ç‰¹æ®Šå¤„ç†

    ```python
            # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
            extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)

            # 6.1 Prepare micro-conditions.
            added_cond_kwargs = {"resolution": None, "aspect_ratio": None}
            if self.transformer.config.sample_size == 128:
                resolution = torch.tensor([height, width]).repeat(
                    batch_size * num_images_per_prompt, 1
                )
                aspect_ratio = torch.tensor([float(height / width)]).repeat(
                    batch_size * num_images_per_prompt, 1
                )
                resolution = resolution.to(dtype=prompt_embeds.dtype, device=device)
                aspect_ratio = aspect_ratio.to(dtype=prompt_embeds.dtype, device=device)

    **#! ---------------------------------------- MODIFIED BELOW ----------------------------------------**
                if (
                    do_classifier_free_guidance
                    and get_classifier_free_guidance_world_size() == 1
                ):
                    resolution = torch.cat([resolution, resolution], dim=0)
                    aspect_ratio = torch.cat([aspect_ratio, aspect_ratio], dim=0)

                #! ORIGIN
                # if do_classifier_free_guidance:
                #     resolution = torch.cat([resolution, resolution], dim=0)
                #     aspect_ratio = torch.cat([aspect_ratio, aspect_ratio], dim=0)
    **#! ---------------------------------------- MODIFIED ABOVE ----------------------------------------**
    ```

4. æ¨¡å‹forwardè¿‡ç¨‹éœ€è¦åœ¨å‰å‡ ä¸ªdiffusion stepä½¿ç”¨åŒæ­¥æµæ°´çº¿åšä¸äººï¼Œåé¢éƒ½ä½¿ç”¨å¼‚æ­¥æµæ°´çº¿ã€‚å¤æ‚çš„é€šä¿¡é€»è¾‘å·²å°è£…è¿›`xFuserPipelineBaseWrapper`ï¼Œç›´æ¥è°ƒç”¨å³å¯
    - è‹¥åœ¨åŸºç±»ä¸­å®ç°çš„`_sync_pipeline`ä¸`_async_pipeline`å‡½æ•°ä¸æ¨¡å‹ä¸é€‚é…ï¼Œåˆ™éœ€è¦åœ¨å½“å‰ç±»ä¸­é‡è½½è¯¥å‡½æ•°ï¼Œå¹¶å‚è€ƒåŸºç±»ä¸­çš„ä»£ç å•ç‹¬å®ç°ã€‚é€šå¸¸è¿™ç§æƒ…å†µä¼šå‡ºç°åœ¨å­˜åœ¨å¤šä½™çš„é€šä¿¡é€»è¾‘æ—¶

    ```python
            # 7. Denoising loop
            num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)
    **#! ---------------------------------------- MODIFIED BELOW ----------------------------------------**
            num_pipeline_warmup_steps = get_runtime_state().runtime_config.warmup_steps

            with self.progress_bar(total=num_inference_steps) as progress_bar:
                if (
                    get_pipeline_parallel_world_size() > 1
                    and len(timesteps) > num_pipeline_warmup_steps
                ):
                    # * warmup stage
                    latents = self._sync_pipeline(
                        latents=latents,
                        prompt_embeds=prompt_embeds,
                        prompt_attention_mask=prompt_attention_mask,
                        guidance_scale=guidance_scale,
                        timesteps=timesteps[:num_pipeline_warmup_steps],
                        num_warmup_steps=num_warmup_steps,
                        extra_step_kwargs=extra_step_kwargs,
                        added_cond_kwargs=added_cond_kwargs,
                        progress_bar=progress_bar,
                        callback=callback,
                        callback_steps=callback_steps,
                    )
                    # * pipefusion stage
                    latents = self._async_pipeline(
                        latents=latents,
                        prompt_embeds=prompt_embeds,
                        prompt_attention_mask=prompt_attention_mask,
                        guidance_scale=guidance_scale,
                        timesteps=timesteps[num_pipeline_warmup_steps:],
                        num_warmup_steps=num_warmup_steps,
                        extra_step_kwargs=extra_step_kwargs,
                        added_cond_kwargs=added_cond_kwargs,
                        progress_bar=progress_bar,
                        callback=callback,
                        callback_steps=callback_steps,
                    )
                else:
                    latents = self._sync_pipeline(
                        latents=latents,
                        prompt_embeds=prompt_embeds,
                        prompt_attention_mask=prompt_attention_mask,
                        guidance_scale=guidance_scale,
                        timesteps=timesteps,
                        num_warmup_steps=num_warmup_steps,
                        extra_step_kwargs=extra_step_kwargs,
                        added_cond_kwargs=added_cond_kwargs,
                        progress_bar=progress_bar,
                        callback=callback,
                        callback_steps=callback_steps,
                        sync_only=True,
                    )
    **#! ---------------------------------------- MODIFIED ABOVE ----------------------------------------**
    ```

5. è¾“å‡ºå¤„ç†ï¼Œç”±äºåªæœ‰æµæ°´çº¿æœ€åä¸€æ®µæŒæœ‰ç”Ÿæˆçš„ç»“æœï¼Œè®¾ç½®ä¸ºä»…æœ‰æ¯ä¸ªdp groupçš„æœ€åä¸€ä¸ªrankè¿”å›æ•°æ®ï¼Œå…¶ä»–rankè¿”å›None

    ```python
            # 8. Decode latents (only rank 0)
    **#! ---------------------------------------- ADD BELOW ----------------------------------------**
            if is_dp_last_rank():
    **#! ---------------------------------------- ADD ABOVE ----------------------------------------**
                if not output_type == "latent":
                    image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]
                    if use_resolution_binning:
                        image = self.image_processor.resize_and_crop_tensor(image, orig_width, orig_height)
                else:
                    image = latents

                if not output_type == "latent":
                    image = self.image_processor.postprocess(image, output_type=output_type)

                # Offload all models
                self.maybe_free_model_hooks()

                if not return_dict:
                    return (image,)

                return ImagePipelineOutput(images=image)
    **#! ---------------------------------------- ADD BELOW ----------------------------------------
            else:**
                return None
    **#! ---------------------------------------- ADD ABOVE ----------------------------------------**
    ```


è‡³æ­¤ï¼Œpipelineä¸­çš„æ”¹åŠ¨å·²å®Œæˆï¼Œåœ¨pipelineçš„__call__å±‚æ¬¡ä¸»è¦å¤„ç†äº†cfgçš„split batchæƒ…å†µã€‚pipeline parallelç›¸å…³çš„æ”¹åŠ¨ä¸é€šä¿¡è¢«å°è£…åˆ°äº†_sync_pipelineä¸_async_pipelineä¸­ï¼Œä»è€Œç®€åŒ–æ¨¡å‹ä¿®æ”¹ã€‚ä½†åœ¨åŸºç±»ä¸­æ­¤å‡½æ•°æ— æ³•æ»¡è¶³æ¨¡å‹éœ€æ±‚æ˜¯åŒæ ·éœ€è¦é‡è½½å¹¶æ‰‹åŠ¨æ›´æ”¹ä»¥ä¿è¯æ­£ç¡®æ€§ã€‚

# 2.transformer backbone class

transformer wrapperæ–‡ä»¶ç›®å½•ä½äº`xfuser/model_executor/models/transformers`ï¼Œåœ¨å…¶ä¸­æ–°å»ºdiffuserä¸­transformer backboneåŒåçš„æ–‡ä»¶å³å¯ã€‚æ­¤ä¾‹ä¸­transformerä¸º`PixArtTransformer2DModel`ï¼Œåœ¨diffusersä¸­ä½äº`pixart_transformer_2d.py`æ–‡ä»¶ä¸­ï¼Œæ•…è¯¥wrapperæ–‡ä»¶åä¸º`xfuser/model_executor/models/transformers/pixart_transformer_2d.py`

transformer backboneæ¨¡å‹åŒæ ·éœ€è¦ç»è¿‡ä¸€å®šçš„ä¿®æ”¹ï¼Œä½†éœ€ä¿®æ”¹å¤„å¾ˆå°‘ï¼Œä¸”ä»…æ¶‰åŠåˆ°å¯¹ç‰¹å®špp_rankåšçš„äº‹æƒ…è¿›è¡Œç‰¹åˆ¤ï¼Œéœ€è¦ä½¿ç”¨`@xFuserTransformerWrappersRegister.register`è£…é¥°å™¨ã€‚å’Œå®ç°ä¸¤ä¸ªå‡½æ•°ï¼Œ`__init__`ä¸`__forward__` æˆ‘ä»¬åé¢åˆ†åˆ«ä»‹ç»ã€‚

```python
@xFuserTransformerWrappersRegister.register(PixArtTransformer2DModel)
class xFuserPixArtTransformer2DWrapper(xFuserTransformerBaseWrapper):
    def __init__(...

    @xFuserBaseWrapper.forward_check_condition
    def forward(...
```

## 2.1.`__init__` ä¿®æ”¹

`__init__`ä¸­éœ€è¦æŒ‡å®štransformer modelä¸­éœ€è¦wrapå“ªäº›å±‚ï¼Œä»¥åŠwrapæ—¶æœ‰æ²¡æœ‰ä»€ä¹ˆé¢å¤–å‚æ•°ã€‚

```python
    def __init__(
        self,
        transformer: PixArtTransformer2DModel,
    ):
        super().__init__(
            transformer=transformer,
            submodule_classes_to_wrap=[nn.Conv2d, PatchEmbed],
            submodule_name_to_wrap=["attn1"],
        )
```

- ä¼ å…¥éœ€è¦wrapçš„layer class(`submodule_classes_to_wrap`)æˆ–è€…å…¶submodule name(`submodule_name_to_wrap`)å³å¯ã€‚é€šå¸¸æ¥è¯´ä¸éœ€è¦æ”¹åŠ¨

## 2.2.`__forward__` ä¿®æ”¹

`__forward__`ä»ç„¶åªéœ€è¦å¯¹diffusers/transformersä¸­åŸå§‹æ¨¡å‹çš„forwardåšå¦‚ä¸‹å°‘è®¸ä¿®æ”¹ï¼Œè¯»è€…è¯·è‡ªè¡Œå¯¹æ¯”æ³¨é‡Šæ‰å’Œæ–°å¢çš„éƒ¨åˆ†ã€‚

1. æ›´æ”¹è·å–height / widthçš„æ–¹å¼ï¼Œå› ä¸ºpatchæƒ…å†µä¸‹æ— æ³•ç›´æ¥é€šè¿‡hidden_stateè·å–åˆ°å‡†ç¡®çš„height & widthã€‚
2. è®¾ç½®ä»…pp_rankä¸º0æ—¶å€™è¿›è¡Œpos_embed

    ```python
            # 1. Input
            batch_size = hidden_states.shape[0]
    **#! ---------------------------------------- MODIFIED BELOW ----------------------------------------**
            #* get height & width from runtime state
            height, width = self._get_patch_height_width()
            #* only pp rank 0 needs pos_embed (patchify)
            if is_pipeline_first_stage():
                hidden_states = self.pos_embed(hidden_states)

            #! ORIGIN
            # height, width = (
            #     hidden_states.shape[-2] // self.config.patch_size,
            #     hidden_states.shape[-1] // self.config.patch_size,
            # )
            # hidden_states = self.pos_embed(hidden_states)
    **#! ---------------------------------------- MODIFIED ABOVE ----------------------------------------**
    ```

3. æ¯ä¸ªdiffusion stepç»“æŸéœ€è¦è¿›è¡Œunpatchifyï¼Œå°†attentionä¸­ä½¿ç”¨çš„tokenså½¢å¼çš„hidden stateè½¬åŒ–å›åˆ°latent spaceä¸‹çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬åªè®©æœ€åä¸€ä¸ªpp_rankåšè¿™ä¸ªæ“ä½œã€‚

    ```python
            # 3. Output
            #* only the last pp rank needs unpatchify
    **#! ---------------------------------------- ADD BELOW ----------------------------------------**
            if is_pipeline_last_stage():
    **#! ---------------------------------------- ADD ABOVE ----------------------------------------**
                shift, scale = (
                    self.scale_shift_table[None] + embedded_timestep[:, None].to(self.scale_shift_table.device)
                ).chunk(2, dim=1)
                hidden_states = self.norm_out(hidden_states)
                # Modulation
                hidden_states = hidden_states * (1 + scale.to(hidden_states.device)) + shift.to(hidden_states.device)
                hidden_states = self.proj_out(hidden_states)
                hidden_states = hidden_states.squeeze(1)

                # unpatchify
                hidden_states = hidden_states.reshape(
                    shape=(-1, height, width, self.config.patch_size, self.config.patch_size, self.out_channels)
                )
                hidden_states = torch.einsum("nhwpqc->nchpwq", hidden_states)
                output = hidden_states.reshape(
                    shape=(-1, self.out_channels, height * self.config.patch_size, width * self.config.patch_size)
                )
    **#! ---------------------------------------- ADD BELOW ----------------------------------------**
            else:
                output = hidden_states
    **#! ---------------------------------------- ADD ABOVE ----------------------------------------**
    ```


# 3.scheduler

schedulerä¸ºæ¯ä¸€ä¸ªdiffusion stepçš„ç»“æœè¿›è¡Œé‡‡æ ·ï¼Œscheduleræœ‰å¾ˆå¤šç§ï¼Œæ¯”å¦‚DDIMï¼ŒDPMç­‰ç­‰ã€‚å¯¹äºä¸åŒschedulerï¼Œæˆ‘ä»¬éƒ½ä»…éœ€å¯¹schedulerç±»çš„ä¸€ä¸ªæˆå‘˜å‡½æ•°`step`è¿›è¡Œä¿®æ”¹ã€‚

åœ¨ç›®å‰çš„æ¡†æ¶ä¸­ï¼Œå·²ç»å®ç°äº†ä¸»æµçš„schedulerçš„å¹¶è¡ŒåŒ–æ”¹é€ ï¼Œè‹¥ä½¿ç”¨æœªå®ç°çš„schedulerï¼Œè¿è¡Œæ—¶ä¼šæœ‰

`ValueError: Scheduler is not supported by xFuser`æŠ¥é”™ï¼Œéœ€è¦å•ç‹¬æ·»åŠ å¯¹schedulerçš„æ”¯æŒã€‚

ç¤ºä¾‹æ–‡ä»¶ä½äº`xfuser/model_executor/schedulers/scheduling_dpmsolver_multistep.py`ï¼Œæ–‡ä»¶ä¸­å¯¹åº”ä½ç½®æ ‡æœ‰ä¿®æ”¹è®°å·ï¼Œå¯å°è¯•ç›´æ¥å°†å¯¹åº”é€»è¾‘æ¬è¿åˆ°æ–°å¢çš„schedulerå³å¯ã€‚

è¯¥éƒ¨åˆ†é€»è¾‘æ˜¯å¯¹äºpatchæƒ…å†µä¸‹çš„model_outputè¿›è¡Œæš‚å­˜ï¼Œé€šè¿‡å¯¹ä¸€ä¸ªå®Œæ•´tensorè¿›è¡Œåˆ‡ç‰‡æ¥æ›´æ–°å¯¹åº”çš„patchä½ç½®ï¼Œä»è€Œåšåˆ°ä¸å•è®¾å¤‡è¿è¡Œæ—¶ç­‰ä»·ã€‚

# 4.layers

DiTä¸­éœ€è¦å¹¶è¡Œæ”¹é€ çš„Layerï¼ˆtorch.nn.Moduleæ´¾ç”Ÿç±»ï¼‰ä¸»è¦é›†ä¸­åœ¨Attention Layerï¼Œæ¯”å¦‚SelfAttentionã€‚å¦‚æœä½¿ç”¨U-Netå·ç§¯ä¹Ÿéœ€è¦å¹¶è¡Œæ”¹é€ ï¼Œä¸è¿‡DiTä¸­å¾ˆå°‘ä½¿ç”¨å·ç§¯ã€‚

Layerçš„æ”¹é€ éœ€è¦å¤„ç†StaleçŠ¶æ€ï¼Œæ¯”å¦‚Attentionä¸­çš„Stale KVï¼Œè¿™éƒ¨åˆ†é€»è¾‘å¤æ‚ï¼Œå’ŒPipeFusionã€Sequence Parallelçš„é€»è¾‘è€¦åˆã€‚å¦‚æœæ¨¡å‹éœ€è¦æ·»åŠ å…¶ä»–layerçš„æƒ…å†µï¼Œè¯·å‚ç…§`xfuser/model_executor/layers`ç›®å½•ä¸­å·²æœ‰layersè¿›è¡Œæ›´æ”¹ã€‚

ç›®å‰æ¡†æ¶ä¸­å·²å¯¹`Conv2d`ï¼Œ`PatchEmbed` layerè¿›è¡Œäº†æ”¯æŒï¼Œå®ƒä»¬è¢«ç”¨åˆ°transformer backboneä¸­çš„`pos_embed`å±‚ä¸å…¶å†…éƒ¨éœ€è¦ç”¨åˆ°çš„å·ç§¯æ“ä½œã€‚diffuseråº“ä¸­attentionå®ç°åç«¯ä¼šæœ‰ä¸åŒAttentionçš„å…·ä½“å®ç°ï¼Œç§°ä¹‹ä¸ºprocessorï¼Œå·¥ä½œé‡åŸå› æ— æ³•ä¸€æ¬¡æ€§å®Œæˆå¯¹äºæ‰€æœ‰processorsçš„æ›´æ”¹ã€‚ç›®å‰å·²å®ç°çš„processorsæœ‰`AttnProcessor2_0`ä¸`JointAttnProcessor2_0`çš„å¹¶è¡ŒåŒ–ç‰ˆæœ¬

ä¸€ä¸ªæ–°çš„æ¨¡å‹åŠ å…¥å¯èƒ½éœ€è¦æ–°çš„processorå®ç°æ”¯æŒã€‚è‹¥æ–°åŠ å…¥æ¨¡å‹çš„Attention processorsä¸è¢«æ”¯æŒï¼Œä¼šå‡ºç°è¿è¡Œæ—¶æŠ¥é”™: `ValueError: Attention Processor class xxx is not supported by xFuser`ã€‚å¦‚æœé‡åˆ°æ­¤ç§æƒ…å†µï¼Œè¯·å°è¯•æ‰§è¡Œå®Œæˆæ”¯æŒæˆ–åœ¨ä»£ç ä»“åº“ä¸­æissueï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å°½å¿«è·å¾—æ”¯æŒã€‚

ç”±äºä½äºæ¨¡å‹ä¸­ä¸åŒä½ç½®ä¸åŒlayerçš„å¹¶è¡ŒåŒ–æ–¹æ³•ä¸åŒï¼Œæ— æ³•åšåˆ°ç»Ÿä¸€ã€‚è‹¥æœ‰ä»»ä½•é—®é¢˜ï¼Œå’¨è¯¢xDiT maintainerã€‚

<aside>
ğŸ’¡ ä¸Šè¿°æ‰€æœ‰ä¿®æ”¹æ ‡è®°å‡åœ¨xDiTé¡¹ç›®pixart-alphaç›¸å…³æºç æ–‡ä»¶ä¸­å­˜åœ¨ï¼Œå»ºè®®ç›´æ¥å‚ç…§å…¶ä¸­çš„ä¿®æ”¹æ ‡è®°è¿›è¡Œæ–°æ¨¡å‹çš„é€‚é…

</aside>